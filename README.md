# HandShake: AI-Powered Sign Language Translator

![ASL Translation](https://img.shields.io/badge/ASL-Translation-blue)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.4+-orange)
![React](https://img.shields.io/badge/React-18.3+-blue)
![Python](https://img.shields.io/badge/Python-3.8+-green)

HandShake is a web application that uses computer vision and machine learning to recognize and translate American Sign Language (ASL) gestures in real-time. The project consists of a React frontend and a Python Flask backend.

## ğŸš€ Performance Metrics

Our model achieves impressive performance on ASL gesture recognition:

- **Accuracy**: 96.88% (training) and 93.04% (validation)
- **Loss**: 0.2410 (training) and 0.2090 (validation)

## âœ¨ Features

- Real-time sign language detection and translation
- Webcam integration for capturing hand gestures
- Speech synthesis for vocalizing detected signs
- Adjustable confidence threshold for detection accuracy
- Visual feedback with detection confidence scores
- Cross-platform compatibility (works in modern browsers)

## ğŸ—ï¸ Project Structure

```
HandShake/
â”œâ”€â”€ backend/               # Python Flask backend
â”‚   â”œâ”€â”€ api.py             # REST API endpoints
â”‚   â”œâ”€â”€ data_loader.py     # Dataset loading utilities
â”‚   â”œâ”€â”€ inference.py       # Model inference code
â”‚   â”œâ”€â”€ model.py           # CNN model architecture
â”‚   â”œâ”€â”€ requirements.txt   # Python dependencies
â”‚   â”œâ”€â”€ train.py           # Model training script
â”‚   â””â”€â”€ webcam_demo.py     # Standalone webcam demo
â”œâ”€â”€ public/                # Static assets
â”œâ”€â”€ src/                   # React frontend
â”‚   â”œâ”€â”€ pages/             # React page components
â”‚   â”œâ”€â”€ services/          # API service layer
â”‚   â”œâ”€â”€ App.tsx            # Main App component
â”‚   â”œâ”€â”€ index.css          # Global styles
â”‚   â””â”€â”€ main.tsx           # Application entry point
â”œâ”€â”€ index.html             # HTML entry point
â”œâ”€â”€ package.json           # NPM dependencies
â”œâ”€â”€ tsconfig.json          # TypeScript configuration
â””â”€â”€ vite.config.ts         # Vite configuration
```

## ğŸ“‹ Prerequisites

- Node.js (v18+)
- Python (v3.8+)
- Webcam
- TensorFlow/Keras (for model training)

## ğŸ› ï¸ Setup and Installation

### Backend Setup

1. Install Python dependencies:

```bash
cd backend
pip install -r requirements.txt
```

2. Train the ASL detection model (if you don't have a pre-trained model):

```bash
python train.py --data_dir path/to/asl_dataset --epochs 20 --img_size 64
```

3. Start the Flask API server:

```bash
python api.py --port 5000
```

### Frontend Setup

1. Install Node.js dependencies:

```bash
npm install
```

2. Update browserslist database (to fix warnings):

```bash
npx update-browserslist-db@latest
```

3. Start the development server:

```bash
npm run dev
```

## ğŸ“± Usage

1. Open your browser and navigate to `http://localhost:5173/`
2. Allow camera access when prompted
3. Position your hand in front of the camera and make ASL signs
4. The application will display the detected letters and build words as you sign
5. Adjust the confidence threshold slider if needed for better detection

## ğŸ§  Training Your Own Model

To train your own sign language detection model, you'll need a dataset of ASL hand gestures. The dataset should be organized with subdirectories for each letter/sign, each containing image examples:

```
asl_dataset/
â”œâ”€â”€ A/
â”‚   â”œâ”€â”€ img_001.jpg
â”‚   â”œâ”€â”€ img_002.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ B/
â”‚   â”œâ”€â”€ img_001.jpg
â”‚   â”œâ”€â”€ img_002.jpg
â”‚   â””â”€â”€ ...
â””â”€â”€ ...
```

Then run the training script:

```bash
python backend/train.py --data_dir path/to/asl_dataset --epochs 20
```

## ğŸ” Model Architecture

Our model uses a CNN architecture with:
- 3 convolutional blocks with increasing filter sizes (32 â†’ 64 â†’ 128)
- Batch normalization for faster training
- Dropout layers to prevent overfitting
- Data augmentation during training
- Early stopping to prevent overfitting

## ğŸ› Troubleshooting

- **API Connection Errors**: Ensure the Flask server is running on port 5000
- **Webcam Issues**: Check browser permissions and ensure no other application is using the webcam
- **Model Training Problems**: Verify your dataset structure and ensure TensorFlow is properly installed
- **Low Detection Accuracy**: Try adjusting the confidence threshold or improving lighting conditions

## ğŸ“š Resources

- [ASL Fingerspelling Alphabet](https://www.nidcd.nih.gov/health/american-sign-language)
- [TensorFlow Documentation](https://www.tensorflow.org/guide)
- [React Documentation](https://reactjs.org/docs/getting-started.html)
- [Flask Documentation](https://flask.palletsprojects.com/)

## ğŸ“„ License

MIT

## ğŸ‘ Acknowledgements

- TensorFlow/Keras for machine learning capabilities
- React and Vite for frontend framework
- Flask for backend API
